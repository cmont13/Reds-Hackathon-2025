### PCA
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load processed dataset
file_path = "C:/Users/Nicol/OneDrive/Desktop/Reds Hackathon/processed_baseball_data.csv"
df = pd.read_csv(file_path)

# Define features (batters + pitchers)
batterX_list = [
    'w_avg_lnch_spd_wo_risp_bat', 'w_avg_lnch_spd_w_risp_bat',
    'w_avg_lnch_ang_wo_risp_bat', 'w_avg_lnch_ang_w_risp_bat',
    'w_avg_est_ba_wo_risp_bat', 'w_avg_est_ba_w_risp_bat',
    'w_avg_est_woba_wo_risp_bat', 'w_avg_est_woba_w_risp_bat',
    'w_avg_iso_value_wo_risp_bat', 'w_avg_iso_value_w_risp_bat',
    'w_avg_hit_distance_wo_risp_bat', 'w_avg_hit_distance_w_risp_bat',
    'ten_days_off_2021_2022_bat', 'ten_days_off_2022_2023_bat',
    'games_played_2021_2022_bat', 'games_played_2022_2023_bat',
    'appearances_2021_bat', 'appearances_2022_bat', 'appearances_2023_bat',
    'avg_appearances_2021_2022_bat', 'trend_appearances_2021_2022_bat',
    'height_bat', 'weight_bat',
    'age_at_start_of_2023_bat', 'years_in_mlb_by_2023_bat',
    'obp_2021_bat', 'obp_2022_bat', 'slg_2021_bat', 'slg_2022_bat',
    'ops_2021_bat', 'ops_2022_bat'
]

pitcherX_list = [
    'appearances_2021_pit', 'appearances_2022_pit', 'appearances_2023_pit',
    'avg_appearances_2021_2022_pit', 'trend_appearances_2021_2022_pit',
    'age_at_start_of_2023_pit', 'years_in_mlb_by_2023_pit'
]

# Combine batter & pitcher features
feature_columns = batterX_list + pitcherX_list

# Target variable (total playing time)
target_variable = "total_playing_time"

# Drop rows with missing values
df = df[['player'] + feature_columns + [target_variable]].dropna()

# Split into train/test sets (keep player IDs)
X_train, X_test, y_train, y_test, player_train, player_test = train_test_split(
    df[feature_columns], df[target_variable], df['player'], test_size=0.2, random_state=42
)

# Standardize features (PCA requires scaling)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA
pca = PCA(n_components=10)  # Choose # of components based on variance explained
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

# Train regression model on PCA-transformed data
model_pca = LinearRegression()
model_pca.fit(X_train_pca, y_train)

# Predict playing time
y_pred_pca = model_pca.predict(X_test_pca)

# Save predictions with player IDs
output_predictions_pca = pd.DataFrame({"player": player_test, "Actual_Playing_Time": y_test, "Predicted_Playing_Time": y_pred_pca})
output_file_path_pca = "C:/Users/Nicol/OneDrive/Desktop/Reds Hackathon/predicted_playing_time_pca.csv"
output_predictions_pca.to_csv(output_file_path_pca, index=False)

print(f"PCA-based Predictions saved to: {output_file_path_pca}")

# Evaluate model performance
print("PCA Model Performance:")
print("MAE:", mean_absolute_error(y_test, y_pred_pca))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_pca)))
print("R^2 Score:", r2_score(y_test, y_pred_pca))

# Show explained variance by each PCA component
explained_variance = pd.DataFrame({
    "Component": range(1, len(pca.explained_variance_ratio_) + 1),
    "Explained Variance": np.cumsum(pca.explained_variance_ratio_)
})
print("\nExplained Variance by PCA Components:")
print(explained_variance)



### Stepwise
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load processed dataset
file_path = "C:/Users/Nicol/OneDrive/Desktop/Reds Hackathon/processed_baseball_data.csv"
df = pd.read_csv(file_path)

# Define features for batters and pitchers
batterX_list = [
    'w_avg_lnch_spd_wo_risp_bat', 'w_avg_lnch_spd_w_risp_bat',
    'w_avg_lnch_ang_wo_risp_bat', 'w_avg_lnch_ang_w_risp_bat',
    'w_avg_est_ba_wo_risp_bat', 'w_avg_est_ba_w_risp_bat',
    'w_avg_est_woba_wo_risp_bat', 'w_avg_est_woba_w_risp_bat',
    'w_avg_iso_value_wo_risp_bat', 'w_avg_iso_value_w_risp_bat',
    'w_avg_hit_distance_wo_risp_bat', 'w_avg_hit_distance_w_risp_bat',
    'ten_days_off_2021_2022_bat', 'ten_days_off_2022_2023_bat',
    'games_played_2021_2022_bat', 'games_played_2022_2023_bat',
    'avg_appearances_2021_2022_bat', 'trend_appearances_2021_2022_bat',
    'height_bat', 'weight_bat',
    'age_at_start_of_2023_bat', 'years_in_mlb_by_2023_bat',
    'obp_2021_bat', 'obp_2022_bat', 'slg_2021_bat', 'slg_2022_bat',
    'ops_2021_bat', 'ops_2022_bat'
]

pitcherX_list = [
    'avg_appearances_2021_2022_pit', 'trend_appearances_2021_2022_pit',
    'age_at_start_of_2023_pit', 'years_in_mlb_by_2023_pit'
]

# Combine batter & pitcher features
feature_columns = batterX_list + pitcherX_list

# Target variable (total playing time)
target_variable = "total_playing_time"

# Drop rows with missing values
df = df[['player'] + feature_columns + [target_variable]].dropna()

# **Remove Features That Directly Sum to Total Playing Time (Data Leakage)**
if "appearances_2021_bat" in df.columns:
    df = df.drop(columns=["appearances_2021_bat", "appearances_2022_bat", "appearances_2023_bat"], errors="ignore")

if "appearances_2021_pit" in df.columns:
    df = df.drop(columns=["appearances_2021_pit", "appearances_2022_pit", "appearances_2023_pit"], errors="ignore")

# Split into train/test sets (keep player IDs)
X_train, X_test, y_train, y_test, player_train, player_test = train_test_split(
    df[feature_columns], df[target_variable], df['player'], test_size=0.2, random_state=42
)

# **Reset indices to align endog (y_train) and exog (X_train)**
X_train = X_train.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)

# **Remove Highly Correlated Features (to Stabilize Stepwise Selection)**
corr_matrix = X_train.corr().abs()
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.99)]

print(f"Removing {len(high_corr_features)} highly correlated features: {high_corr_features}")
X_train = X_train.drop(columns=high_corr_features)
X_test = X_test.drop(columns=high_corr_features)

# **Standardize Features** (Important for Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert back to DataFrame for Stepwise Regression
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

# **Stepwise Regression Function**
def stepwise_selection(X, y, initial_features=[], threshold_in=0.01, threshold_out=0.05, verbose=True, max_iterations=50):
    """
    Perform a forward-backward stepwise regression algorithm.
    Adds features with p < threshold_in and removes with p > threshold_out.
    Stops if no more improvement or if max_iterations is reached.
    """
    included = list(initial_features)
    iteration = 0  # Track iterations
    
    while iteration < max_iterations:
        changed = False

        # **Forward Step: Try adding each feature not yet included**
        excluded = list(set(X.columns) - set(included))
        new_pval = pd.Series(dtype=float, index=excluded)
        for new_column in excluded:
            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()
            new_pval[new_column] = model.pvalues[new_column]
        best_pval = new_pval.min() if not new_pval.empty else None
        if best_pval is not None and best_pval < threshold_in:
            best_feature = new_pval.idxmin()
            included.append(best_feature)
            changed = True
            if verbose:
                print(f'Adding feature: {best_feature} (p={best_pval:.4f})')

        # **Backward Step: Remove features with p > threshold_out**
        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()
        pvalues = model.pvalues.iloc[1:]  # Ignore intercept p-value
        worst_pval = pvalues.max()
        if worst_pval > threshold_out:
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
            changed = True
            if verbose:
                print(f'Removing feature: {worst_feature} (p={worst_pval:.4f})')

        iteration += 1  # Increment iteration count
        if not changed:
            break  # Stop if no more changes

    print(f"Stepwise Selection Completed in {iteration} iterations.")
    return included

# **Apply Stepwise Selection**
selected_features = stepwise_selection(X_train_scaled_df, y_train)

# **Train Final Model Using Selected Features**
X_train_selected = X_train_scaled_df[selected_features]
X_test_selected = X_test_scaled_df[selected_features]

final_model = sm.OLS(y_train, sm.add_constant(X_train_selected)).fit()
y_pred_stepwise = final_model.predict(sm.add_constant(X_test_selected))

# **Save Predictions with Player IDs**
output_predictions_stepwise = pd.DataFrame({
    "player": player_test,
    "Actual_Playing_Time": y_test,
    "Predicted_Playing_Time": y_pred_stepwise
})
output_file_path_stepwise = "C:/Users/Nicol/OneDrive/Desktop/Reds Hackathon/predicted_playing_time_stepwise.csv"
output_predictions_stepwise.to_csv(output_file_path_stepwise, index=False)

print(f"\nStepwise Regression Predictions saved to: {output_file_path_stepwise}")
print("\nFinal Selected Features:")
print(selected_features)

# **Evaluate Model Performance**
print("\nStepwise Regression Model Performance:")
print("MAE:", mean_absolute_error(y_test, y_pred_stepwise))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_stepwise)))
print("R^2 Score:", r2_score(y_test, y_pred_stepwise))



### Lasso
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Load processed dataset
file_path = "C:/Users/Nicol/OneDrive/Desktop/Reds Hackathon/processed_baseball_data.csv"
df = pd.read_csv(file_path)

# Define **ONLY independent features** (No appearances, games played, or direct time indicators)
feature_columns = [
    'w_avg_lnch_spd_wo_risp_bat', 'w_avg_lnch_spd_w_risp_bat',
    'w_avg_lnch_ang_wo_risp_bat', 'w_avg_lnch_ang_w_risp_bat',
    'w_avg_est_ba_wo_risp_bat', 'w_avg_est_ba_w_risp_bat',
    'w_avg_est_woba_wo_risp_bat', 'w_avg_est_woba_w_risp_bat',
    'w_avg_iso_value_wo_risp_bat', 'w_avg_iso_value_w_risp_bat',
    'w_avg_hit_distance_wo_risp_bat', 'w_avg_hit_distance_w_risp_bat',
    'ten_days_off_2021_2022_bat', 'ten_days_off_2022_2023_bat',
    'height_bat', 'weight_bat',
    'age_at_start_of_2023_bat', 'years_in_mlb_by_2023_bat',
    'obp_2021_bat', 'obp_2022_bat', 'slg_2021_bat', 'slg_2022_bat',
    'ops_2021_bat', 'ops_2022_bat',
    'age_at_start_of_2023_pit', 'years_in_mlb_by_2023_pit'
]

# Target variable
target_variable = "total_playing_time"

# Drop rows with missing values in key columns
df = df[['player'] + feature_columns + [target_variable]].dropna()

# Split into training and test sets (80% training, 20% test)
X_train, X_test, y_train, y_test, player_train, player_test = train_test_split(
    df[feature_columns], df[target_variable], df['player'], test_size=0.2, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Lasso Regression model with cross-validation
lasso = LassoCV(cv=5, alphas=np.logspace(-4, 4, 100))
lasso.fit(X_train_scaled, y_train)

# Predict playing time
y_pred = lasso.predict(X_test_scaled)

# Save predictions with player IDs
output_predictions = pd.DataFrame({"player": player_test, "Actual_Playing_Time": y_test, "Predicted_Playing_Time": y_pred})
output_file_path = "C:/Users/Nicol/OneDrive/Desktop/Reds Hackathon/predicted_playing_time_no_leakage.csv"
output_predictions.to_csv(output_file_path, index=False)

print(f"Predictions saved to: {output_file_path}")

# Compute and print model evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("\nModel Performance Metrics (No Data Leakage):")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"RÂ² Score: {r2:.4f}")

# Display selected features and coefficients
selected_features = np.array(feature_columns)[np.abs(lasso.coef_) > 1e-4]
feature_importance = pd.DataFrame({"Feature": selected_features, "Coefficient": lasso.coef_[np.abs(lasso.coef_) > 1e-4]})
feature_importance = feature_importance.sort_values(by="Coefficient", ascending=False)

# Show feature importance
print("\nSelected Features and Coefficients:")

### Logistic Regression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc

# Load data
savantdata = pd.read_csv("C:/Users/Nicol/OneDrive/Desktop/Reds Hackathon/savant_data_2021_2023.csv")
lahman_people = pd.read_csv("C:/Users/Nicol/OneDrive/Desktop/Reds Hackathon/lahman_people.csv")

# Select relevant columns and filter data
logistic_data = savantdata[['pitcher', 'batter', 'release_speed', 'release_pos_x', 'release_pos_z', 
                             'balls', 'strikes', 'p_throws', 'description']]

logistic_data = logistic_data[logistic_data['description'].isin(["called_strike", "ball"])]
logistic_data['strike_result'] = (logistic_data['description'] == "called_strike").astype(int)

# Remove missing values
logistic_data_clean = logistic_data.dropna(subset=['release_speed', 'release_pos_x', 'release_pos_z', 
                                                   'balls', 'strikes', 'p_throws'])

# Convert categorical variable 'p_throws' using one-hot encoding
logistic_data_clean = pd.get_dummies(logistic_data_clean, columns=['p_throws'], drop_first=True)

# Define features and target variable
X = logistic_data_clean[['release_speed', 'release_pos_x', 'release_pos_z', 'balls', 'strikes', 'p_throws_R']]
y = logistic_data_clean['strike_result']

# Train logistic regression model
model = LogisticRegression(solver='liblinear')
model.fit(X, y)

# Model summary
print("Intercept:", model.intercept_)
print("Coefficients:", model.coef_)

# Predict probabilities and classifications
logistic_data_clean['predicted_prob'] = model.predict_proba(X)[:, 1]
logistic_data_clean['predicted_class'] = (logistic_data_clean['predicted_prob'] > 0.5).astype(int)

# Confusion matrix
conf_matrix = confusion_matrix(y, logistic_data_clean['predicted_class'])
print("Confusion Matrix:\n", conf_matrix)

# Classification report
report = classification_report(y, logistic_data_clean['predicted_class'])
print("Classification Report:\n", report)

# ROC Curve
fpr, tpr, _ = roc_curve(y, logistic_data_clean['predicted_prob'])
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

# Calculate accuracy, precision, recall, and F1-score
accuracy = (logistic_data_clean['predicted_class'] == y).mean()
precision = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[0, 1]) if (conf_matrix[1, 1] + conf_matrix[0, 1]) > 0 else 0
recall = conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 0]) if (conf_matrix[1, 1] + conf_matrix[1, 0]) > 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")
print(feature_importance)

#Testing Various Models
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_validate, KFold
from sklearn import preprocessing
from sklearn.feature_selection import RFECV, r_regression
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor, HistGradientBoostingRegressor, StackingRegressor
from sklearn.metrics import root_mean_squared_error
from sklearn.linear_model import Lasso, LinearRegression
import xgboost as xgb
import tensorflow as tf
import tensorflow_decision_forests as tfdf
import keras

data = pd.read_csv('totalsDF.csv')
sample = pd.read_csv('sample_submission.csv')
sample_players = sample['PLAYER_ID']

sum_stats = data.groupby('player')[
    [  'appearances_2021', 'appearances_2022',
       'appearances_2023', 'avg_appearances_2021_2022',
       'avg_appearances_2022_2023', 'trend_appearances_2021_2022',
       'trend_appearances_2022_2023',
       'Avg Pitch Speed', 'Max Pitch Speed', 'Avg Effective Speed',
       'Max Effective Speed', 'Avg Spin Rate', 'Max Spin Rate',
       'Mean Exit Velocity', 'Max Exit Velocity', 'w_avg_lnch_spd_wo_risp',
       'w_avg_lnch_spd_w_risp', 'w_avg_lnch_ang_wo_risp',
       'w_avg_lnch_ang_w_risp', 'w_avg_est_ba_wo_risp', 'w_avg_est_ba_w_risp',
       'w_avg_est_woba_wo_risp', 'w_avg_est_woba_w_risp',
       'w_avg_iso_value_wo_risp', 'w_avg_iso_value_w_risp',
       'w_avg_hit_distance_wo_risp', 'w_avg_hit_distance_w_risp',
       'w_avg_lnch_spd_wo_risp_pctl', 'w_avg_lnch_spd_w_risp_pctl',
       'w_avg_lnch_ang_wo_risp_pctl', 'w_avg_lnch_ang_w_risp_pctl',
       'w_avg_est_ba_wo_risp_pctl', 'w_avg_est_ba_w_risp_pctl',
       'w_avg_est_woba_wo_risp_pctl', 'w_avg_est_woba_w_risp_pctl',
       'w_avg_iso_value_wo_risp_pctl', 'w_avg_iso_value_w_risp_pctl',
       'w_avg_hit_distance_wo_risp_pctl', 'w_avg_hit_distance_w_risp_pctl',
       'avg_ff_velo', 'sp_indicator', '2021', '2022', '2023', 'change_2022',
       'change_2023']
].sum().reset_index()

single_stats = data.groupby('player')[
    [  'height', 'weight',
       'age_at_start_of_2023', 'age_at_start_of_2024', 'years_in_mlb_by_2023',
       'years_in_mlb_by_2024','ten_days_off_2021_2022',
       'ten_days_off_2022_2023', 'games_played_2021_2022',
       'games_played_2022_2023']
].max().reset_index()

stats = ['obp_2021', 'obp_2022', 'obp_2023',
     'slg_2021', 'slg_2022', 'slg_2023',
     'ops_2021', 'ops_2022', 'ops_2023']
season_stats = data.pivot(index='player', columns='role', values=stats).reset_index()
season_stats.columns = ['player', 'obp_2021_p', 'obp_2021_b', 'obp_2022_p', 'obp_2022_b', 'obp_2023_p', 'obp_2023_b',
                        'slg_2021_p', 'slg_2021_b', 'slg_2022_p', 'slg_2022_b', 'slg_2023_p', 'slg_2023_b',
                        'ops_2021_p', 'ops_2021_b', 'ops_2022_p', 'ops_2022_b', 'ops_2023_p', 'ops_2023_b']

new_data = sum_stats.merge(single_stats, on='player')
new_data = new_data.merge(season_stats, on='player')

new_data['pitcher'] = ((new_data['ops_2021_p'] > 0) | (new_data['ops_2022_p'] > 0) | (new_data['ops_2023_p'] > 0)).astype(int)
new_data['batter'] = ((new_data['ops_2021_b'] > 0) | (new_data['ops_2022_b'] > 0) | (new_data['ops_2023_b'] > 0)).astype(int)

X = np.array(new_data[['w_avg_lnch_spd_wo_risp_pctl',
       'w_avg_lnch_spd_w_risp_pctl', 'w_avg_lnch_ang_wo_risp_pctl',
       'w_avg_lnch_ang_w_risp_pctl', 'w_avg_est_ba_wo_risp_pctl',
       'w_avg_est_ba_w_risp_pctl', 'w_avg_est_woba_wo_risp_pctl',
       'w_avg_est_woba_w_risp_pctl', 'w_avg_iso_value_wo_risp_pctl',
       'w_avg_iso_value_w_risp_pctl', 'w_avg_hit_distance_wo_risp_pctl',
       'w_avg_hit_distance_w_risp_pctl',
       'ten_days_off_2021_2022', 'games_played_2021_2022',
       'appearances_2021', 'appearances_2022',
       'avg_appearances_2021_2022', 'trend_appearances_2021_2022',
       'height', 'weight', 'age_at_start_of_2023', 'years_in_mlb_by_2023',
       'obp_2021_p', 'obp_2022_p', 'slg_2021_p', 'slg_2022_p',
       'ops_2021_p', 'ops_2022_p',
       'obp_2021_b', 'obp_2022_b', 'slg_2021_b', 'slg_2022_b',
       'ops_2021_b', 'ops_2022_b',
       'Avg Pitch Speed', 'Max Pitch Speed', 'Avg Effective Speed',
       'Max Effective Speed', 'Avg Spin Rate', 'Max Spin Rate',
       '2021', '2022', 'change_2022', 'pitcher', 'batter'
       ]])

y = np.array(new_data['appearances_2023'])

#selector = RFECV(GradientBoostingRegressor(), cv=5)
#selector = selector.fit(X, y)

X = selector.transform(X)

models = (
    LinearRegression(),
    Lasso(),
    RandomForestRegressor(),
    tfdf.keras.RandomForestModel(
      task=tfdf.keras.Task.REGRESSION),
    GradientBoostingRegressor(),
    HistGradientBoostingRegressor(),
    xgb.XGBRegressor(objective="reg:squarederror", random_state=42),
    AdaBoostRegressor()

)

for m in models:
  rmse = []

  for train, test in KFold(n_splits=5).split(X):
    X_train = X[train]
    y_train = y[train]
    X_test = X[test]
    y_test = y[test]

    model = m

    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    rmse.append(root_mean_squared_error(y_test, y_pred))

  print('AVERAGE RMSE:', np.array(rmse).mean())

#Final Model
model = tfdf.keras.RandomForestModel(
      task=tfdf.keras.Task.REGRESSION)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

model.fit(X_train, y_train)

X_2024 = np.array(new_data[['w_avg_lnch_spd_wo_risp_pctl',
       'w_avg_lnch_spd_w_risp_pctl', 'w_avg_lnch_ang_wo_risp_pctl',
       'w_avg_lnch_ang_w_risp_pctl', 'w_avg_est_ba_wo_risp_pctl',
       'w_avg_est_ba_w_risp_pctl', 'w_avg_est_woba_wo_risp_pctl',
       'w_avg_est_woba_w_risp_pctl', 'w_avg_iso_value_wo_risp_pctl',
       'w_avg_iso_value_w_risp_pctl', 'w_avg_hit_distance_wo_risp_pctl',
       'w_avg_hit_distance_w_risp_pctl',
       'ten_days_off_2022_2023', 'games_played_2022_2023',
       'appearances_2022', 'appearances_2023',
       'avg_appearances_2022_2023', 'trend_appearances_2022_2023',
       'height', 'weight', 'age_at_start_of_2024', 'years_in_mlb_by_2024',
       'obp_2022_p', 'obp_2023_p', 'slg_2022_p', 'slg_2023_p',
       'ops_2022_p', 'ops_2023_p',
       'obp_2022_b', 'obp_2023_b', 'slg_2022_b', 'slg_2023_b',
       'ops_2022_b', 'ops_2023_b',
       'Avg Pitch Speed', 'Max Pitch Speed', 'Avg Effective Speed',
       'Max Effective Speed', 'Avg Spin Rate', 'Max Spin Rate',
       '2022', '2023', 'change_2023', 'pitcher', 'batter'
       ]])

X_2024 = selector.transform(X_2024)

y_2024 = model.predict(X_2024)
pred = new_data[['player']].merge(pd.DataFrame(y_2024), left_index=True, right_index=True)
pred.rename(columns={'player':'PLAYER_ID', 0:'PLAYING_TIME'}, inplace=True)
submission = pred[pred['PLAYER_ID'].isin(sample_players)]

submission.to_csv('submission.csv', index=False)
